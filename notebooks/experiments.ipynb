{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from data_loader.data_generator import DataGenerator\n",
    "from models.gan import GAN\n",
    "from trainers.gan_trainer import GANTrainer\n",
    "from utils.config import process_config\n",
    "from utils.logger import Logger\n",
    "from utils.utils import get_args\n",
    "from utils.dirs import create_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show image patches from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "\n",
    "from utils.config import process_config\n",
    "config_file = 'configs/gan_test.json'\n",
    "config = process_config(config_file)\n",
    "\n",
    "data = DataGenerator(config)\n",
    "\n",
    "dataset = data.dataset\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "init = tf.global_variables_initializer()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(iterator.initializer)\n",
    "    images = sess.run(next_element)\n",
    "    size = 10\n",
    "    fig = plt.figure(figsize=(size, size))\n",
    "    plt.title(\"Training Images\")\n",
    "    for i in range(size * size):\n",
    "        plt.subplot(size, size, i+1)\n",
    "        plt.imshow(1 -images[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    #print(images[0])\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from models.gan_model import GAN\n",
    "import numpy as np\n",
    "mod = GAN(config)\n",
    "with tf.Session() as sess:\n",
    "    mod.load(sess)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "noise = tf.random_normal([config.batch_size,config.noise_dim])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    noise_gen = sess.run(noise)\n",
    "    feed_dict = {mod.noise_input: noise_gen}\n",
    "    generator_predictions = sess.run([mod.progress_images], feed_dict=feed_dict)\n",
    "    gen_p = np.asarray(generator_predictions)\n",
    "    print(gen_p.shape)\n",
    "#     size = 4\n",
    "#     fig = plt.figure(figsize=(size*2, size*2))\n",
    "#     plt.title(\"Training Images\")\n",
    "#     for i in range(size * size):\n",
    "#         plt.subplot(size, size, i+1)\n",
    "#         plt.imshow(gen_p[i, :, :, 0], cmap='gray')\n",
    "#         plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving images from Summary Object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "import imageio as io\n",
    "\n",
    "def save_images_from_event(fn, tag, output_dir='outputs'):\n",
    "    assert(os.path.isdir(output_dir))\n",
    "\n",
    "    image_str = tf.placeholder(tf.string)\n",
    "    im_tf = tf.image.decode_image(image_str)\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    with sess.as_default():\n",
    "        count = 0\n",
    "        for e in tf.train.summary_iterator(fn):\n",
    "            for v in e.summary.value:\n",
    "                if v.tag.find(tag) !=-1:\n",
    "                    im = im_tf.eval({image_str: v.image.encoded_image_string})\n",
    "                    output_fn = os.path.realpath('{}/image_{:05d}.png'.format(output_dir, count))\n",
    "                    print(\"Saving '{}'\".format(output_fn))\n",
    "                    io.imwrite(output_fn, im)\n",
    "                    count += 1  \n",
    "#save_images_from_event('./Logs/gan_test_new_noise/summary/train/events.out.tfevents.1553277049.Yigits-MacBook-Pro.local','FromNoise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making gif from generated image samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os\n",
    "from utils.dirs import listdir_nohidden\n",
    "def make_gif_from_images(folder_name,output_name):\n",
    "    files = listdir_nohidden(folder_name)\n",
    "    images = []\n",
    "    for filename in files:\n",
    "        images.append(imageio.imread(folder_name+\"/\"+filename))\n",
    "    imageio.mimsave(output_name + \".gif\", images)\n",
    "make_gif_from_images(\"Logs/gan_test_new_noise/generated\",\"./gan_test_new_noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.dense = tf.keras.layers.Dense(4)\n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    print('Training', training)\n",
    "    return self.dense(inputs)\n",
    "    \n",
    "model = MyModel()\n",
    "model.compile(optimizer=tf.train.AdagradOptimizer(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "inp = np.ones((5, 3), dtype=np.float32)\n",
    "out = np.ones((5, 4), dtype=np.float32)\n",
    "\n",
    "# training should be False\n",
    "model(inp)\n",
    "\n",
    "# training should be True\n",
    "model.fit(inp, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import process_config\n",
    "config_file = 'configs/gan_test.json'\n",
    "config = process_config(config_file,\"mogofogo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALAD Model Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment parameters are already stored\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "init = tf.global_variables_initializer()\n",
    "from models.gan_tf import GAN_TF\n",
    "from utils.config import process_config\n",
    "config_file = 'configs/gan_tf.json'\n",
    "config = process_config(config_file,\"gan_model_test\")\n",
    "mod = GAN_TF(config)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #mod.load(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'DCGAN/Generator/g_bn_1/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator/g_bn_1/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator/g_bn_2/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator/g_bn_2/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator/g_bn_3/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator/g_bn_3/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator/g_bn_4/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator/g_bn_4/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Discriminator/d_bn_1/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Discriminator/d_bn_1/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Discriminator/d_bn_2/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Discriminator/d_bn_2/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Discriminator/d_bn_3/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Discriminator/d_bn_3/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Discriminator_1/d_bn_1/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Discriminator_1/d_bn_1/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Discriminator_1/d_bn_2/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Discriminator_1/d_bn_2/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Discriminator_1/d_bn_3/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Discriminator_1/d_bn_3/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator_1/g_bn_1/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator_1/g_bn_1/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator_1/g_bn_2/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator_1/g_bn_2/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator_1/g_bn_3/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator_1/g_bn_3/AssignMovingAvg_1' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator_1/g_bn_4/AssignMovingAvg' type=AssignSub>,\n",
       " <tf.Operation 'DCGAN/Generator_1/g_bn_4/AssignMovingAvg_1' type=AssignSub>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'DCGAN/Generator/g_dense/kernel:0' shape=(100, 12544) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Generator/g_bn_1/gamma:0' shape=(12544,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Generator/g_bn_1/beta:0' shape=(12544,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Generator/g_conv2dtr_1/kernel:0' shape=(5, 5, 128, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Generator/g_bn_2/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Generator/g_bn_2/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Generator/g_conv2dtr_2/kernel:0' shape=(5, 5, 128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Generator/g_bn_3/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Generator/g_bn_3/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Generator/g_conv2dtr_3/kernel:0' shape=(5, 5, 128, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Generator/g_bn_4/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Generator/g_bn_4/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Generator/g_conv2dtr_4/kernel:0' shape=(5, 5, 1, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_conv1/kernel:0' shape=(5, 5, 1, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_conv1/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_bn_1/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_bn_1/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_conv_2/kernel:0' shape=(5, 5, 128, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_conv_2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_bn_2/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_bn_2/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_conv_3/kernel:0' shape=(5, 5, 64, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_conv_3/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_bn_3/gamma:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_bn_3/beta:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_dense/kernel:0' shape=(1568, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'DCGAN/Discriminator/d_dense/bias:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.gen_update_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.disc_update_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in mod.generator.layers: print(layer.get_config(), layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.learning_phase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from models.alad import ALAD\n",
    "from utils.config import process_config\n",
    "config_file = 'configs/alad.json'\n",
    "config = process_config(config_file,\"mogofogo\")\n",
    "mod = ALAD(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.encoder.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.encoder.updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(mod.discriminator_xz,\"test.png\",show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.discriminator_xz.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.discriminator_xx.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.discriminator_zz.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.encoder.updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.layers.BatchNormalization(trainab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Execution GAN Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from models.gan_keras import GAN_keras\n",
    "from utils.config import process_config\n",
    "config_file = 'configs/gan_keras.json'\n",
    "config = process_config(config_file,\"keras_test\")\n",
    "mod = GAN_keras(config)\n",
    "with tf.Session() as sess:\n",
    "    mod.load(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from models.gan_eager import GAN_eager\n",
    "from utils.config import process_config\n",
    "config_file = 'configs/gan_eager.json'\n",
    "config = process_config(config_file,\"eager_test\")\n",
    "mod = GAN_eager(config)\n",
    "mod.load()\n",
    "# with tf.Session() as sess:\n",
    "#      mod.load(sess)\n",
    "#for layer in mod.generator.layers: print(layer.get_config(), layer.get_weights())\n",
    "#noise = tf.random.normal([1, 100])\n",
    "#generated_image = mod.generator(noise, training=False)\n",
    "\n",
    "#plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers =mod.generator.layers\n",
    "for layer in layers: print(layer.get_config(), layer.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod.discriminator.summary()\n",
    "noise = tf.random.normal([1, 28,28,1])\n",
    "real_out = mod.discriminator(noise,training=False)\n",
    "print(real_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eager Execution Dataset Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "from data_loader.data_generator_eager import DataGenerator_eager\n",
    "\n",
    "from utils.config import process_config\n",
    "config_file = 'configs/gan_eager.json'\n",
    "config = process_config(config_file,\"dataset\")\n",
    "\n",
    "data = DataGenerator_eager(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.contrib.eager.Iterator(data.dataset)\n",
    "    #out = mod.discriminator(da.numpy(),training=False)\n",
    "    #print(da.numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from time import sleep\n",
    "t =tqdm([\"a\", \"b\", \"c\", \"d\"])\n",
    "for i in t:\n",
    "    t.set_description(\"Epoch:{} iteration:{}\".format(1,i))\n",
    "    t.refresh() # to show immediately the update\n",
    "    sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "pbar = \n",
    "for char in pbar:\n",
    "    pbar.set_description(\"Processing %s\" % char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google DCGAN Eager Implementation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow.keras.layers as layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "      \n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)  \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)    \n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', \n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "      \n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "       \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.losses.sigmoid_cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = tf.losses.sigmoid_cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return tf.losses.sigmoid_cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "discriminator_optimizer = tf.train.AdamOptimizer(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):  \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "    \n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as we go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "    \n",
    "    # Save the model every 15 epochs\n",
    "       # if (epoch + 1) % 15 == 0:\n",
    "        #checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "    \n",
    "  # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset,EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "import tensorflow as tf\n",
    "n_units = 10\n",
    "in_training_mode = tf.placeholder(tf.bool)\n",
    "X = tf.keras.layers.Input(shape=(100,))\n",
    "hidden = tf.keras.layers.Dense(n_units,\n",
    "                               activation=None)(X) # no activation function, yet\n",
    "batch_normed = tf.keras.layers.BatchNormalization()(hidden,training=True)\n",
    "output = tf.keras.activations\\\n",
    "           .relu(batch_normed) # ReLu is typically done after batch normalization\n",
    "\n",
    "# optimizer code here â€¦\n",
    "\n",
    "extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#with tf.control_dependencies(extra_ops):\n",
    "#    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_custom_estimator(features, labels, mode, params):\n",
    "    in_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    use_batch_norm = params['batch_norm']\n",
    "    \n",
    "    net = tf.feature_column.input_layer(features, params['features'])\n",
    "    for i, n_units in enumerate(params['hidden_units']):\n",
    "        net = build_fully_connected(net, n_units=n_units, training=in_training, \n",
    "                                    batch_normalization=use_batch_norm, \n",
    "                                    activation=params['activation'],\n",
    "                                    name='hidden_layer'+str(i))\n",
    "    \n",
    "    logits = output_layer(net, 10, batch_normalization=use_batch_norm,\n",
    "                          training=in_training)\n",
    "    \n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)\n",
    "    accuracy = tf.metrics.accuracy(labels=tf.argmax(labels, 1),\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    tf.summary.scalar('accuracy', accuracy[1])  # for visualizing in TensorBoard\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss,\n",
    "                                          eval_metric_ops={'accuracy': accuracy})\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    extra_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])\n",
    "    with tf.control_dependencies(extra_ops):\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(output_dir):\n",
    "    features = [tf.feature_column.numeric_column(key='image_data', shape=(28*28))]\n",
    "    classifier = tf.estimator.Estimator(model_fn=dnn_custom_estimator,\n",
    "                                        model_dir=output_dir,\n",
    "                                        params={'features': features,\n",
    "                                                'batch_norm': USE_BATCH_NORMALIZATION,\n",
    "                                                'activation': ACTIVATION,\n",
    "                                                'hidden_units': HIDDEN_UNITS,\n",
    "                                                'learning_rate': LEARNING_RATE})\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=NUM_STEPS)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\n",
    "    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)\n",
    "\n",
    "train_and_evaluate('mnist_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "is_training = tf.placeholder_with_default(False, (), 'is_training')\n",
    "x = tf.placeholder(tf.float32, [None, 1], 'x')\n",
    "y = tf.layers.batch_normalization(x, training=is_training)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    y = tf.identity(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "x_1 = [[-10], [0], [10]]\n",
    "x_2 = [[-10]]\n",
    "for _ in range(1000):\n",
    "    y_1 = sess.run(y, feed_dict={x: x_1, is_training: True})\n",
    "y_2 = sess.run(y, feed_dict={x: x_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"\", reuse=tf.AUTO_REUSE):\n",
    "    out = sess.run([tf.get_variable('batch_normalization/moving_mean'),\n",
    "                    tf.get_variable('batch_normalization/moving_variance')])\n",
    "    moving_average, moving_variance = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.reset_default_graph()\n",
    "noise_tensor = tf.placeholder(\n",
    "                tf.float32, shape=[None, 100], name=\"noise\"\n",
    "            )\n",
    "            # Densely connected Neural Network layer with 12544 Neurons.\n",
    "x_g = tf.layers.Dense(units=7 * 7 * 256, use_bias=False,\n",
    "                        kernel_initializer=tf.truncated_normal_initializer(stddev=0.02))(noise_tensor)\n",
    "x_g = tf.layers.batch_normalization(inputs=x_g,momentum=0.8,training=True)\n",
    "x_g = tf.nn.leaky_relu(features=x_g,alpha=0.5)\n",
    "x_g = tf.reshape(x_g,shape=[-1,7, 7, 256])\n",
    "x_g = tf.layers.Conv2DTranspose(filters=128, kernel_size=5, strides=(1, 1), padding=\"same\", use_bias=False,\n",
    "                                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.02))(x_g)\n",
    "assert x_g.get_shape().as_list() == [None, 7, 7, 128]\n",
    "\n",
    "x_g = tf.layers.batch_normalization(inputs=x_g,momentum=0.8,training=True)\n",
    "x_g = tf.nn.leaky_relu(features=x_g,alpha=0.5)\n",
    "\n",
    "x_g = tf.layers.Conv2DTranspose(filters=128, kernel_size=5, strides=(2, 2), padding=\"same\", use_bias=False,\n",
    "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.02))(x_g)\n",
    "assert x_g.get_shape().as_list() == [None, 14, 14, 128]\n",
    "\n",
    "x_g = tf.layers.batch_normalization(inputs=x_g,momentum=0.8,training=True)\n",
    "x_g = tf.nn.leaky_relu(features=x_g,alpha=0.5)\n",
    "\n",
    "x_g = tf.layers.Conv2DTranspose(filters=128, kernel_size=5, strides=(2, 2), padding=\"same\", use_bias=False,\n",
    "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.02))(x_g)\n",
    "assert x_g.get_shape().as_list() == [None, 28, 28, 128]\n",
    "x_g = tf.layers.batch_normalization(inputs=x_g,momentum=0.8,training=True)\n",
    "x_g = tf.nn.leaky_relu(features=x_g,alpha=0.5)\n",
    "\n",
    "x_g = tf.layers.Conv2DTranspose(filters=1, kernel_size=5, strides=(1, 1), padding=\"same\", use_bias=False,\n",
    "                                  activation=tf.nn.tanh,\n",
    "                                  kernel_initializer=tf.truncated_normal_initializer(stddev=0.02))(x_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.reset_default_graph()\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(7*7*64, use_bias=False)\n",
    "        self.batchnorm1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv1 = tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False)\n",
    "        self.batchnorm2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = tf.keras.layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False)\n",
    "        self.batchnorm3 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv3 = tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm1(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = tf.reshape(x, shape=(-1, 7, 7, 64))\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm2(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm3(x, training=training)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = tf.nn.tanh(self.conv3(x))  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
